# Data-Science-Portfolio

Data Science Portfolio

A curated collection of data science and analytics projects completed during my undergraduate studies. These projects reflect hands-on experience in data cleaning, exploratory analysis, database design, predictive modeling, and decision-support workflows.

**What This Portfolio Demonstrates**

Across these projects, I focus on building complete, end-to-end workflows rather than isolated scripts. Each project typically includes:

- Data extraction and preprocessing (ETL)
- Cleaning and standardizing inconsistent or messy datasets
- Exploratory data analysis with visualizations
- Feature engineering and encoding
- Predictive modeling with evaluation metrics
- Exportable outputs for reporting or stakeholder use

The goal is not just to build models, but to structure data in a way that supports clear decision-making.

**Core Skills Represented**

- Python (pandas, NumPy, scikit-learn, matplotlib, seaborn)
- SQL and relational database design
- Data normalization (up to 3NF)
- Feature engineering and correlation analysis
- Model training, hyperparameter tuning (GridSearchCV)
- Performance evaluation (RÂ², MAE, RMSE)
- Data storytelling through structured analysis

**Highlighted Projects(more coming)**
Emissions Decision Support System

Built an ETL pipeline to clean and standardize county-level emissions data. Performed trend analysis and implemented a Decision Tree regression model to estimate emissions based on socioeconomic factors. Exported cleaned data for reporting and dashboard integration.

Audio Storage System Migration

Simulated a migration from a flat Microsoft Access export to a normalized Oracle-style schema. Applied 3NF normalization techniques to reduce redundancy and built a regression model to estimate storage costs based on file metadata.

Incarceration Trends Analysis

Analyzed county-level incarceration and recidivism data. Conducted correlation analysis to explore relationships between incarceration rates and socioeconomic indicators. Built a predictive model to estimate recidivism rates and evaluated feature importance for interpretability.

**Approach**

1. My work emphasizes structured problem-solving:

2. Understand the business or policy question.

3. Clean and validate the data.

4. Explore patterns and relationships.

5. Build a baseline model.

6. Evaluate and interpret results.

7. Deliver outputs that are usable by non-technical stakeholders.

**Purpose**

This repository represents my growth as a data practitioner and my ability to connect technical analysis with real-world impact. I am particularly interested in roles at the intersection of analytics, database systems, and applied machine learning.
