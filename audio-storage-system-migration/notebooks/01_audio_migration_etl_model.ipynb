{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Audio Storage System (Access â†’ Oracle Migration)\nThis notebook demonstrates an end-to-end workflow:\n- ingest a flat Access-style export\n- clean inconsistent text fields\n- normalize into lookup tables (3NF-style)\n- run EDA and train a baseline model to estimate storage cost\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "df = pd.read_csv('../data/access_export_audio_records.csv')\ndf.head()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "df.shape, df.dtypes\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data Preprocessing Part 1\nClean the messy department field, standardize categories, and validate obvious ranges.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Standardize department\ndf['department'] = (\n    df['department'].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True).str.title()\n)\n\n# Standardize storage tiers\ndf['storage_tier'] = df['storage_tier'].astype(str).str.strip().str.title()\n\n# Basic range checks\ndf = df[(df['duration_sec'] > 0) & (df['duration_sec'] <= 10*3600)]\ndf = df[(df['file_size_mb'] > 0) & (df['file_size_mb'] < 5000)]\n\ndf.head()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "df.isna().sum()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "df.describe(include='all').T.head(12)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Normalization (3NF-style)\nWe build lookup tables and replace text keys with IDs (similar to how you'd load into Oracle).\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Lookup tables\ndept_lu = df[['department']].drop_duplicates().sort_values('department').reset_index(drop=True)\ndept_lu['dept_id'] = np.arange(1, len(dept_lu)+1)\n\ntype_lu = df[['recording_type']].drop_duplicates().sort_values('recording_type').reset_index(drop=True)\ntype_lu['type_id'] = np.arange(1, len(type_lu)+1)\n\ntier_lu = df[['storage_tier']].drop_duplicates().sort_values('storage_tier').reset_index(drop=True)\ntier_lu['tier_id'] = np.arange(1, len(tier_lu)+1)\n\n# Replace text with IDs\nnorm = df.merge(dept_lu, on='department').merge(type_lu, on='recording_type').merge(tier_lu, on='storage_tier')\n\naudio_files = norm[[\n    'record_id','file_name','dept_id','type_id','tier_id',\n    'duration_sec','sample_rate_hz','channels','file_size_mb','created_date','created_by',\n    'storage_cost_usd_month'\n]].copy()\n\ndept_lu.head(), type_lu.head(), tier_lu.head()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Save normalized outputs (as if staging for Oracle load)\ndept_lu.to_csv('../reports/departments.csv', index=False)\ntype_lu.to_csv('../reports/recording_types.csv', index=False)\ntier_lu.to_csv('../reports/storage_tiers.csv', index=False)\naudio_files.to_csv('../reports/audio_files_normalized.csv', index=False)\n\naudio_files.head()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Exploratory Data Analysis\nWe inspect distributions and relationships that drive storage cost.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "plt.figure(figsize=(8,4))\nsns.histplot(audio_files['storage_cost_usd_month'], bins=40)\nplt.title('Distribution of monthly storage cost')\nplt.show()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "plt.figure(figsize=(8,4))\nsns.scatterplot(data=audio_files.sample(600, random_state=42), x='file_size_mb', y='storage_cost_usd_month')\nplt.title('Storage cost vs file size')\nplt.show()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "plt.figure(figsize=(10,4))\nsns.countplot(x='tier_id', data=audio_files)\nplt.title('Count by storage tier (encoded)')\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Encoding + Correlation Heatmap\nWe use numeric/encoded fields directly for a quick correlation view.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "corr_cols = ['storage_cost_usd_month','duration_sec','sample_rate_hz','channels','file_size_mb','dept_id','type_id','tier_id']\nplt.figure(figsize=(9,6))\nsns.heatmap(audio_files[corr_cols].corr(), annot=True, cmap='coolwarm')\nplt.title('Correlation matrix')\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Outlier Removal + Train/Test Split + Model\nBaseline: DecisionTreeRegressor with GridSearchCV\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from scipy import stats\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n\nmodel_df = audio_files.copy()\n\nz = np.abs(stats.zscore(model_df[['duration_sec','file_size_mb','storage_cost_usd_month']]))\nmodel_df = model_df[(z < 3).all(axis=1)]\n\nX = model_df.drop(columns=['storage_cost_usd_month','file_name','created_date','created_by'])\ny = model_df['storage_cost_usd_month']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nparams = {\n    'max_depth': [3,5,7,9],\n    'min_samples_split': [2,4,8],\n    'min_samples_leaf': [1,2,4],\n    'random_state': [42]\n}\n\ngrid = GridSearchCV(DecisionTreeRegressor(), param_grid=params, cv=5, n_jobs=-1, verbose=0)\ngrid.fit(X_train, y_train)\n\nbest = grid.best_estimator_\nbest.fit(X_train, y_train)\n\npred = best.predict(X_test)\n\nprint(\"Best params:\", grid.best_params_)\nprint(\"R2:\", r2_score(y_test, pred))\nprint(\"MAE:\", mean_absolute_error(y_test, pred))\nprint(\"RMSE:\", np.sqrt(mean_squared_error(y_test, pred)))\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "feat_df = pd.DataFrame({'Feature': X.columns, 'Importance': best.feature_importances_}).sort_values('Importance', ascending=False)\nfeat_df\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "plt.figure(figsize=(8,4))\nsns.barplot(data=feat_df, x='Importance', y='Feature')\nplt.title('Feature Importance')\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Conclusion\nThis project shows:\n- a practical ETL cleaning step (messy text fields)\n- normalization for database loading (3NF-style)\n- baseline cost modeling and feature importance to explain drivers\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}